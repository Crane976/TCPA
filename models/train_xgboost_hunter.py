# models/train_xgboost_hunter.py (UNIVERSAL GPU VERSION)
# é€‚é…: CIC-IDS2017 & CSE-CIC-IDS2018
# äº®ç‚¹: åŠ¨æ€é‡‡æ · + å¼€å¯GPUåŠ é€Ÿ
import pandas as pd
import numpy as np
import os
import sys
import joblib
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import classification_report, f1_score
from tqdm import tqdm

# --- è·¯å¾„è®¾ç½® ---
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

from config import DEFENDER_SET, set_seed

# --- é…ç½®åŒº ---
TRAIN_SET_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
TEST_SET_PATH = os.path.join(project_root, 'data', 'splits', 'holdout_test_set.csv')
SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
MODELS_DIR = os.path.join(project_root, 'models')
HUNTER_MODEL_PATH = os.path.join(MODELS_DIR, 'xgboost_hunter.pkl')


def main():
    set_seed(2025)
    print("=" * 60)
    print("ðŸš€ è®­ç»ƒ XGBoost Hunter (é€šç”¨é€‚é…ç‰ˆ - GPUåŠ é€Ÿ)...")
    print("=" * 60)

    # --- 1. åŠ è½½ä¸Žæ¸…æ´— ---
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    if not os.path.exists(TRAIN_SET_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒé›† {TRAIN_SET_PATH}")
        return

    df_train = pd.read_csv(TRAIN_SET_PATH)
    df_test = pd.read_csv(TEST_SET_PATH)
    scaler = joblib.load(SCALER_PATH)
    feature_names = scaler.feature_names_in_

    # æ¸…æ´— Inf/NaN
    for df in [df_train, df_test]:
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(subset=[col for col in DEFENDER_SET if col in df.columns], inplace=True)

    # --- 2. åˆ’åˆ†ä¸ŽåŠ¨æ€é‡‡æ · ---
    print("\n[æ­¥éª¤1] æž„å»ºè®­ç»ƒå­é›† (Smart Sampling)...")
    X_full = df_train[feature_names]
    y_full = df_train['label']

    # åˆ’åˆ†éªŒè¯é›† (ä¿æŒçœŸå®žåˆ†å¸ƒï¼Œç”¨äºŽé˜ˆå€¼å¯»ä¼˜)
    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
        X_full, y_full, test_size=0.2, random_state=2025, stratify=y_full
    )

    df_pool = pd.concat([X_train_split, y_train_split], axis=1)
    df_bot = df_pool[df_pool['label'] == 1]
    df_benign = df_pool[df_pool['label'] == 0]

    n_bot = len(df_bot)
    n_benign_total = len(df_benign)

    # ðŸ”¥ [ç­–ç•¥å‡çº§] åŠ¨æ€é‡‡æ ·æ¯”ä¾‹
    # å¯¹äºŽæ ‘æ¨¡åž‹ï¼Œä¸éœ€è¦ä¸¥æ ¼ 1:1ã€‚ç»™å®ƒæ›´å¤šè‰¯æ€§æ ·æœ¬(å¦‚ 1:5)èƒ½å‡å°‘è¯¯æŠ¥ã€‚
    # é€»è¾‘: è¯•å›¾å– Bot çš„ 5 å€è‰¯æ€§ï¼Œä½†ç»ä¸è¶…è¿‡è‰¯æ€§æ€»æ•°ã€‚
    TARGET_RATIO = 5
    n_benign_sample = min(n_benign_total, n_bot * TARGET_RATIO)

    df_benign_sampled = df_benign.sample(n=n_benign_sample, random_state=2025)
    df_train_balanced = pd.concat([df_bot, df_benign_sampled])

    print(f"   -> Botæ ·æœ¬: {n_bot}")
    print(f"   -> Benignæ ·æœ¬: {n_benign_sample} (Ratio 1:{n_benign_sample / n_bot:.1f})")
    print(f"   -> æ€»è®­ç»ƒé‡: {len(df_train_balanced)}")

    # --- 3. ç¼©æ”¾ ---
    print("\n[æ­¥éª¤2] æ•°æ®æ ‡å‡†åŒ–...")
    X_train_final = scaler.transform(df_train_balanced[feature_names])
    y_train_final = df_train_balanced['label']

    X_val_scaled = scaler.transform(X_val_split)  # éªŒè¯é›†ä¿æŒçœŸå®žæ¯”ä¾‹
    X_test_scaled = scaler.transform(df_test[feature_names])
    y_test = df_test['label'].values

    # --- 4. å‚æ•°æœç´¢ (GPU åŠ é€Ÿ) ---
    print("\n[æ­¥éª¤3] æ­£åœ¨æœç´¢æœ€ä½³å‚æ•° (RandomizedSearch)...")

    # ðŸ”¥ [å…³é”®] å¼€å¯ GPU åŠ é€Ÿ
    # tree_method='hist', device='cuda' æ˜¯ XGBoost æ–°ç‰ˆå¯ç”¨ GPU çš„æ ‡å‡†å†™æ³•
    xgb_clf = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        tree_method='hist',  # ä½¿ç”¨ç›´æ–¹å›¾ç®—æ³• (æœ€å¿«)
        device='cuda',  # ä½¿ç”¨ GPU (RTX 4060)
        use_label_encoder=False,
        random_state=2025
    )

    param_dist = {
        'n_estimators': [200, 400, 600],
        'max_depth': [6, 10, 14],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0],
        # 'scale_pos_weight': [1, 3] # å¦‚æžœä¸åšé‡‡æ ·ï¼Œå¯ä»¥ç”¨è¿™ä¸ªå‚æ•°å¹³è¡¡ï¼Œä½†æˆ‘ä»¬åšäº†ç‰©ç†é‡‡æ ·
    }

    # æ³¨æ„: n_jobs åœ¨ GPU æ¨¡å¼ä¸‹é€šå¸¸è®¾ä¸º 1 æˆ– -1 å‡å¯ï¼Œä¸»è¦é  GPU ç®—
    search = RandomizedSearchCV(
        xgb_clf, param_dist, n_iter=8, scoring='f1', cv=3, verbose=1, n_jobs=1, random_state=2025
    )

    try:
        search.fit(X_train_final, y_train_final)
    except Exception as e:
        print(f"âš ï¸ GPUè®­ç»ƒå¤±è´¥ (å¯èƒ½æ˜¯æ˜¾å­˜ä¸è¶³æˆ–ç‰ˆæœ¬é—®é¢˜): {e}")
        print("   -> åˆ‡æ¢å›ž CPU æ¨¡å¼ç»§ç»­è®­ç»ƒ...")
        xgb_clf.set_params(device='cpu')
        search.fit(X_train_final, y_train_final)

    best_model = search.best_estimator_
    print(f"   -> æœ€ä½³å‚æ•°: {search.best_params_}")

    # --- 5. é˜ˆå€¼å¯»ä¼˜ ---
    print("\n[æ­¥éª¤4] å¯»æ‰¾æœ€ä½³å†³ç­–é˜ˆå€¼...")
    # ä½¿ç”¨ GPU é¢„æµ‹åŠ é€Ÿ
    val_probs = best_model.predict_proba(X_val_scaled)[:, 1]

    best_thr, best_f1 = 0.5, 0
    thresholds = np.arange(0.1, 0.96, 0.05)

    for thr in thresholds:
        y_pred = (val_probs >= thr).astype(int)
        f1 = f1_score(y_val_split, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_thr = thr

    print(f"âœ… æœ€ä½³é˜ˆå€¼: {best_thr:.2f} (Val F1: {best_f1:.4f})")

    # --- 6. ä¿å­˜ä¸Žæœ€ç»ˆè¯„ä¼° ---
    joblib.dump(best_model, HUNTER_MODEL_PATH)
    print(f"ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜è‡³: {HUNTER_MODEL_PATH}")

    print(f"\n--- 'XGBoost Hunter' æœ€ç»ˆè¯„ä¼° (æµ‹è¯•é›†) ---")
    test_probs = best_model.predict_proba(X_test_scaled)[:, 1]
    y_test_pred = (test_probs >= best_thr).astype(int)

    print(classification_report(y_test, y_test_pred, target_names=['Benign (0)', 'Bot (1)'], digits=4))


if __name__ == "__main__":
    main()