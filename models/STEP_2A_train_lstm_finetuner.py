# models/STEP_2A_train_lstm_finetuner.py
import pandas as pd
import numpy as np
import os
import sys
import joblib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

from config import DEFENDER_SET, ATTACKER_KNOWLEDGE_SET, ATTACKER_ACTION_SET, set_seed
from models.lstm_finetuner import LSTMFinetuner  # å¯¼å…¥æ–°æ¨¡å‹

# --- é…ç½®åŒº ---
TRAIN_SET_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
LSTM_FINETUNER_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_finetuner.pt')  # æ–°æ¨¡å‹è·¯å¾„

# --- æ¨¡å‹å‚æ•° ---
INPUT_DIM = len(ATTACKER_KNOWLEDGE_SET)  # 13ç»´
OUTPUT_DIM = len(ATTACKER_ACTION_SET)  # 9ç»´
HIDDEN_DIM = 64

# --- è®­ç»ƒå‚æ•° ---
EPOCHS = 100
BATCH_SIZE = 128
LEARNING_RATE = 0.001
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def main():
    set_seed(2025)
    print("=" * 60);
    print("ğŸš€ ä¸‰çº§æ¡†æ¶ - STEP 2A: è®­ç»ƒLSTMç²¾è°ƒå™¨ (æˆ˜æœ¯å±‚)...");
    print("=" * 60)
    print(f"   >>> ç›®æ ‡: å­¦ä¹ ä»13ç»´'è®¤çŸ¥é›†'ç²¾ç¡®æ˜ å°„åˆ°9ç»´'è¡ŒåŠ¨é›†' <<<")

    print("\n[æ­¥éª¤1] åŠ è½½æ•°æ®å’ŒScaler...")
    df_train_full = pd.read_csv(TRAIN_SET_PATH)
    scaler = joblib.load(SCALER_PATH)

    print("\n[æ­¥éª¤2] å‡†å¤‡çœŸå®Botæµé‡ç”¨äºè®­ç»ƒ...")
    df_bot_train = df_train_full[df_train_full['label'] == 1].copy()

    # å…ˆå¯¹æ‰€æœ‰Botæµé‡çš„23ç»´ç‰¹å¾è¿›è¡Œç¼©æ”¾
    bot_scaled_full = scaler.transform(df_bot_train[DEFENDER_SET])
    df_bot_scaled = pd.DataFrame(bot_scaled_full, columns=DEFENDER_SET)

    # è¾“å…¥X: 13ç»´è®¤çŸ¥é›† (scaled)
    X_train = df_bot_scaled[ATTACKER_KNOWLEDGE_SET].values
    # è¾“å‡ºY: 9ç»´è¡ŒåŠ¨é›† (scaled)
    Y_train = df_bot_scaled[ATTACKER_ACTION_SET].values

    # ä¸ºLSTMå‡†å¤‡æ•°æ®: [æ ·æœ¬æ•°, åºåˆ—é•¿åº¦=1, ç‰¹å¾ç»´åº¦]
    X_train_seq = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)
    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)

    dataset = TensorDataset(X_train_seq, Y_train_tensor)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæ¯•, ä½¿ç”¨ {len(dataset)} æ¡Botæ ·æœ¬ã€‚")

    print("\n[æ­¥éª¤3] å¼€å§‹è®­ç»ƒLSTMç²¾è°ƒå™¨...")
    model = LSTMFinetuner(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for x_batch, y_batch in loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            pred = model(x_batch)
            loss = criterion(pred, y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        if (epoch + 1) % 10 == 0:
            print(f"  -> Epoch {epoch + 1:3d}/{EPOCHS}, Train Loss: {total_loss / len(loader):.6f}")

    torch.save(model.state_dict(), LSTM_FINETUNER_MODEL_PATH)
    print(f"\nâœ… LSTMç²¾è°ƒå™¨å·²ä¿å­˜åˆ°: {LSTM_FINETUNER_MODEL_PATH}")


if __name__ == "__main__":
    main()