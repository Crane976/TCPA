# generate/STEP3_Variant_B_no_constraint.py
# Ablation Study Variant B: w/o Hard Constraints (No Physical Consistency)
# Adaptive for both CIC-IDS2017 and CSE-CIC-IDS2018

import pandas as pd
import numpy as np
import os
import sys
import joblib
import torch
from sklearn.cluster import KMeans  # ä¿ç•™èšç±»

# ==========================================================
# --- è·¯å¾„ä¿®æ­£ä¸æ¨¡å—å¯¼å…¥ ---
# ==========================================================
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

import config
from models.style_transfer_cae import ConditionalAutoencoder
from models.lstm_finetuner import LSTMFinetuner
from models.lstm_predictor import LSTMPredictor
from config import DEFENDER_SET, ATTACKER_KNOWLEDGE_SET, ATTACKER_ACTION_SET, COMPLEX_SET, set_seed

# ==========================================================
# --- é…ç½®åŒº (åŸºäº Config) ---
# ==========================================================
CLEAN_DATA_PATH = os.path.join(config.SPLITS_DIR, 'training_set.csv')
TEST_DATA_PATH = os.path.join(config.SPLITS_DIR, 'holdout_test_set.csv')

SCALER_PATH = config.SCALER_PATH
MODEL_DIR = config.MODEL_SAVE_DIR
CAE_MODEL_PATH = os.path.join(MODEL_DIR, 'style_transfer_cae.pt')
LSTM_FINETUNER_MODEL_PATH = os.path.join(MODEL_DIR, 'lstm_finetuner.pt')
PREDICTOR_MODEL_PATH = os.path.join(MODEL_DIR, 'lstm_reconciliation_predictor.pt')

# ğŸ”¥ åŠ¨æ€è¾“å‡ºæ–‡ä»¶å
OUTPUT_CSV_NAME = f'variant_B_no_constraint_{config.CURRENT_DATASET}.csv'
OUTPUT_CSV_PATH = os.path.join(project_root, 'data', 'generated', OUTPUT_CSV_NAME)

FEATURE_DIM_CAE = len(ATTACKER_KNOWLEDGE_SET)
LATENT_DIM_CAE = 5
NUM_CLASSES_CAE = 2
INPUT_DIM_LSTM_FINETUNER = len(ATTACKER_KNOWLEDGE_SET)
OUTPUT_DIM_LSTM_FINETUNER = len(ATTACKER_ACTION_SET)
INPUT_DIM_PREDICTOR = len(ATTACKER_ACTION_SET)
OUTPUT_DIM_PREDICTOR = len(COMPLEX_SET)

# --- æˆ˜æœ¯å‚æ•° ---
TACTICAL_SUPPRESSION_RATIO = 100
TACTICAL_WINDOW_CAP_2018 = 1000
MIMIC_INTENSITY = 0.98
NUM_BOT_CLUSTERS = 5
WATERMARK_KEY = 97
WATERMARK_FEATURE = 'Flow Duration'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ==========================================================
# --- è¾…åŠ©å‡½æ•° ---
# ==========================================================
def calculate_adaptive_quantity():
    """è‡ªé€‚åº”è®¡ç®—ç”Ÿæˆæ•°é‡"""
    print(f"\nğŸ” [Variant B] æ­£åœ¨åˆ†ææµ‹è¯•é›†è§„æ¨¡: {config.CURRENT_DATASET} ...")
    if not os.path.exists(TEST_DATA_PATH):
        return 40000

    df_test = pd.read_csv(TEST_DATA_PATH)
    label_col = 'Label' if 'Label' in df_test.columns else 'label'

    if df_test[label_col].dtype == object:
        df_test[label_col] = df_test[label_col].apply(lambda x: 0 if str(x).lower() == 'benign' else 1)

    real_bot_count = len(df_test[df_test[label_col] == 1])

    if config.CURRENT_DATASET == 'CIC-IDS2017':
        target_num = real_bot_count * TACTICAL_SUPPRESSION_RATIO
    elif config.CURRENT_DATASET == 'CSE-CIC-IDS2018':
        target_num = min(real_bot_count, TACTICAL_WINDOW_CAP_2018) * TACTICAL_SUPPRESSION_RATIO
    else:
        target_num = 40000

    print(f"   -> ç›®æ ‡ Bot æ•°: {real_bot_count}")
    print(f"   -> è®¡åˆ’ç”Ÿæˆæ•°: {target_num}")
    return int(target_num)


def inject_watermark_variant_B(df, key, feature_name):
    """
    Variant B ä¸“ç”¨çš„æ°´å°æ³¨å…¥:
    åªä¿®æ”¹ Durationï¼Œ**æ•…æ„ä¸** æ›´æ–° Bytes/s å’Œ Pkts/sã€‚
    è¿™æ ·ä¼šäººä¸ºåˆ¶é€ å‡ºç‰©ç†é€»è¾‘æ¼æ´ï¼Œæ¨¡æ‹Ÿæ²¡æœ‰ç¡¬çº¦æŸçš„æƒ…å†µã€‚
    """
    print(f"\nğŸŒŠ [æ­¥éª¤7] æ³¨å…¥æ°´å° (Variant B Mode)...")
    df_w = df.copy()
    values = df_w[feature_name].values.astype(int)
    residuals = values % key
    new_values = values - residuals
    mask_too_small = (new_values <= 0)
    new_values[mask_too_small] += key
    df_w[feature_name] = new_values

    print("   -> âš ï¸ æ³¨æ„: Variant B ä¸ä¼šåŒæ­¥æ›´æ–°å…³è”ç‰¹å¾ (Rate/Pkts)ï¼Œæ•…æ„ä¿ç•™ä¸è‡ªæ´½æ€§!")
    # è¿™é‡Œæˆ‘ä»¬ä»€ä¹ˆéƒ½ä¸åšï¼Œç›´æ¥è¿”å›ï¼Œè¿™å°±æ˜¯æ¶ˆèå®éªŒçš„ç²¾é«“

    return df_w


# ==========================================================
# --- ä¸»å‡½æ•° ---
# ==========================================================
def main():
    set_seed(2025)
    print("=" * 60)
    print(f"ğŸš€ [æ¶ˆèå®éªŒ Variant B] æ— ç¡¬çº¦æŸ (No Physical Constraints)")
    print(f"   Dataset: {config.CURRENT_DATASET}")
    print("=" * 60)

    # 0. ç¡®å®šæ•°é‡
    NUM_TO_GENERATE = calculate_adaptive_quantity()
    if NUM_TO_GENERATE <= 0: return

    # 1. åŠ è½½æ¨¡å‹ (ä¸å˜)
    print("\n[æ­¥éª¤1] åŠ è½½æ¨¡å‹ä¸æ•°æ®...")
    scaler = joblib.load(SCALER_PATH)
    predictor = LSTMPredictor(INPUT_DIM_PREDICTOR, OUTPUT_DIM_PREDICTOR).to(device)
    try:
        predictor.load_state_dict(torch.load(PREDICTOR_MODEL_PATH, map_location=device))
    except:
        predictor.load_state_dict(torch.load(PREDICTOR_MODEL_PATH, map_location=device))
    predictor.eval()

    cae_model = ConditionalAutoencoder(FEATURE_DIM_CAE, LATENT_DIM_CAE, NUM_CLASSES_CAE).to(device)
    cae_model.load_state_dict(torch.load(CAE_MODEL_PATH, map_location=device))
    cae_model.eval()

    lstm_finetuner = LSTMFinetuner(INPUT_DIM_LSTM_FINETUNER, OUTPUT_DIM_LSTM_FINETUNER).to(device)
    lstm_finetuner.load_state_dict(torch.load(LSTM_FINETUNER_MODEL_PATH, map_location=device))
    lstm_finetuner.eval()

    df_clean_full = pd.read_csv(CLEAN_DATA_PATH)

    # 1.1 å‡†å¤‡ Benign æ¯ä½“
    df_benign_source = df_clean_full[df_clean_full['label'] == 0].sample(n=NUM_TO_GENERATE, replace=True,
                                                                         random_state=2025)

    # 1.2 å‡†å¤‡ Bot å…¨é‡æ•°æ®
    df_bot_all = df_clean_full[df_clean_full['label'] == 1]
    # é’ˆå¯¹ 2018 é‡‡æ ·èšç±»
    if len(df_bot_all) > 20000:
        df_bot_clustering = df_bot_all.sample(n=20000, random_state=2025)
    else:
        df_bot_clustering = df_bot_all

    # 1.5 èšç±» (ä¿ç•™)
    print("\n[æ­¥éª¤1.5] æ‰§è¡Œèšç±»èšç„¦ (Ablation: No, èšç±»ä¿ç•™)...")
    bot_scaled_full = scaler.transform(df_bot_clustering[DEFENDER_SET])
    kmeans = KMeans(n_clusters=NUM_BOT_CLUSTERS, random_state=2025, n_init=10)
    kmeans.fit(bot_scaled_full)
    centers_unscaled = scaler.inverse_transform(kmeans.cluster_centers_)
    df_bot_centers = pd.DataFrame(centers_unscaled, columns=DEFENDER_SET)
    tutor_indices = np.random.randint(0, NUM_BOT_CLUSTERS, size=NUM_TO_GENERATE)
    df_bot_tutors = df_bot_centers.iloc[tutor_indices].reset_index(drop=True)

    # 2. é£æ ¼æ¤å…¥ (ä¸å˜)
    print("\n[æ­¥éª¤2] TIER 1: æ‰§è¡Œç‚¹å¯¹ç‚¹é£æ ¼æ¤å…¥...")
    with torch.no_grad():
        source_scaled = scaler.transform(df_benign_source[DEFENDER_SET])
        k_indices = [DEFENDER_SET.index(c) for c in ATTACKER_KNOWLEDGE_SET]

        X_benign_full = torch.tensor(source_scaled, dtype=torch.float32).to(device)
        X_benign = X_benign_full[:, k_indices]
        c_benign = torch.tensor([1.0, 0.0], dtype=torch.float32).expand(len(X_benign), -1).to(device)
        z_benign = cae_model.encode(X_benign, c_benign)

        tutors_scaled = scaler.transform(df_bot_tutors[DEFENDER_SET])
        X_bot_full = torch.tensor(tutors_scaled, dtype=torch.float32).to(device)
        X_bot = X_bot_full[:, k_indices]
        c_bot_input = torch.tensor([0.0, 1.0], dtype=torch.float32).expand(len(X_bot), -1).to(device)
        z_bot = cae_model.encode(X_bot, c_bot_input)

        z_hybrid = (1 - MIMIC_INTENSITY) * z_benign + MIMIC_INTENSITY * z_bot
        c_bot_target = torch.tensor([0.0, 1.0], dtype=torch.float32).expand(len(z_hybrid), -1).to(device)
        generated_knowledge_features_scaled = cae_model.decode(z_hybrid, c_bot_target)

    # 3. LSTM (ä¸å˜)
    print("\n[æ­¥éª¤3] TIER 2: LSTM å¾®è°ƒ...")
    with torch.no_grad():
        input_for_lstm = generated_knowledge_features_scaled.unsqueeze(1)
        refined_action = lstm_finetuner(input_for_lstm)
        fused_action = np.clip(refined_action.cpu().numpy(), 0, 1)

    # 4. é¢„æµ‹ (ä¸å˜)
    print("\n[æ­¥éª¤4] TIER 3: è¡ç”Ÿç‰¹å¾é¢„æµ‹...")
    with torch.no_grad():
        input_predictor = torch.tensor(fused_action, dtype=torch.float32).unsqueeze(1).to(device)
        predicted_complex = predictor(input_predictor).cpu().numpy()
        predicted_complex = np.clip(predicted_complex, 0, 1)

    # 5. é€†å‘ç¼©æ”¾ (ä¸å˜)
    print("\n[æ­¥éª¤5] é€†å‘ç¼©æ”¾...")
    X_gen_full = np.zeros((NUM_TO_GENERATE, len(DEFENDER_SET)))

    for i, col in enumerate(ATTACKER_ACTION_SET):
        col_idx = DEFENDER_SET.index(col)
        X_gen_full[:, col_idx] = fused_action[:, i]

    for i, col in enumerate(COMPLEX_SET):
        col_idx = DEFENDER_SET.index(col)
        X_gen_full[:, col_idx] = predicted_complex[:, i]

    X_gen_original = scaler.inverse_transform(X_gen_full)
    df_final = pd.DataFrame(X_gen_original, columns=DEFENDER_SET)

    # ------------------------------------------------------------------
    # âŒ æ ¸å¿ƒæ¶ˆèç‚¹: ç§»é™¤ç¡¬çº¦æŸ (No Hard Constraints)
    # ------------------------------------------------------------------
    print("\n[æ­¥éª¤6] âŒ è·³è¿‡ç‰©ç†ç¡¬çº¦æŸæ ¡å‡† (Ablation: No Constraints)...")
    print("   -> ç›´æ¥ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹çš„åŸå§‹å€¼ (å­˜åœ¨ Rate != Total/Duration é€»è¾‘æ¼æ´)")

    # ä»…è¡¥å…¨ç¼ºå¤±åˆ— (Calculable Set åœ¨ Tier-123 ä¸­æ²¡æœ‰é¢„æµ‹)
    # é‚£äº›æ²¡æœ‰è¢« LSTM é¢„æµ‹çš„ç‰¹å¾ (æ¯”å¦‚ Flow Bytes/s)ï¼Œæˆ‘ä»¬å¿…é¡»ç»™ä¸€ä¸ªå€¼ï¼Œå¦åˆ™ Scaler æŠ¥é”™ã€‚
    # ä¸ºäº†ä½“ç°"æ— çº¦æŸ"ï¼Œæˆ‘ä»¬ç»™å®ƒä»¬èµ‹ 0ï¼Œæˆ–è€…éšæœºæ•°ï¼Œæˆ–è€…ä¿ç•™ NaN (å¦‚æœè¯„ä¼°è„šæœ¬èƒ½å¤„ç†)
    # æœ€å…¬å¹³çš„åšæ³•ï¼šä¸è®¡ç®—ã€‚å¦‚æœå¿…é¡»è®¡ç®—ï¼Œå°±ç”¨é”™è¯¯çš„å…¬å¼ç®— (æ¯”å¦‚åªé™¤ä»¥1ï¼Œä¸é™¤ä»¥Duration)
    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©è¡¥ 0ï¼Œæ¨¡æ‹Ÿæ”»å‡»è€…å¿˜è®°å¤„ç†è¿™äº›ä¾èµ–ç‰¹å¾ã€‚
    for col in DEFENDER_SET:
        if col not in df_final.columns:
            df_final[col] = 0

    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    df_final = df_final[DEFENDER_SET]

    # 7. æ°´å° (ä¿®æ”¹ç‰ˆï¼Œä¸æ›´æ–°å…³è”ç‰¹å¾)
    df_final_watermarked = inject_watermark_variant_B(df_final, WATERMARK_KEY, WATERMARK_FEATURE)
    df_final_watermarked['Label'] = 1

    df_final_watermarked.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"\nâœ… Variant B (No Constraint) ç”Ÿæˆå®Œæ¯•: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    main()