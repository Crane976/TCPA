# generate/STEP3_generate_with_3tier.py (FINAL PURE DEEP LEARNING VERSION)
import pandas as pd
import numpy as np
import os
import sys
import joblib
import torch

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

# âœ… 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å‹
from models.style_transfer_cae import ConditionalAutoencoder
from models.lstm_finetuner import LSTMFinetuner
from models.lstm_predictor import LSTMPredictor  # å¯¼å…¥æ–°çš„LSTMé¢„æµ‹å™¨
from config import DEFENDER_SET, ATTACKER_KNOWLEDGE_SET, ATTACKER_ACTION_SET, set_seed

# --- é…ç½®åŒº ---
# è¾“å…¥
BENIGN_SOURCE_PATH = os.path.join(project_root, 'data', 'filtered', 'benign_traffic.csv')
SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
TRAIN_SET_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
CAE_MODEL_PATH = os.path.join(project_root, 'models', 'style_transfer_cae.pt')
LSTM_FINETUNER_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_finetuner.pt')
# âœ… 2. ä¿®æ”¹: åŠ è½½æ–°çš„LSTMé¢„æµ‹å™¨æ¨¡å‹
PREDICTOR_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_reconciliation_predictor.pt')

# è¾“å‡º
# âœ… 3. ä¿®æ”¹: ä½¿ç”¨æ–°çš„è¾“å‡ºæ–‡ä»¶åä»¥ä½œåŒºåˆ†
OUTPUT_CSV_PATH = os.path.join(project_root, 'data', 'generated', 'final_camouflage_bot_3tier_lstm.csv')

# æ¨¡å‹å‚æ•°
FEATURE_DIM_CAE = len(ATTACKER_KNOWLEDGE_SET)
LATENT_DIM_CAE = 5
NUM_CLASSES_CAE = 2
INPUT_DIM_LSTM_FINETUNER = len(ATTACKER_KNOWLEDGE_SET)
OUTPUT_DIM_LSTM_FINETUNER = len(ATTACKER_ACTION_SET)
# âœ… 4. æ–°å¢: LSTMé¢„æµ‹å™¨çš„å‚æ•°
INPUT_DIM_PREDICTOR = len(ATTACKER_ACTION_SET)
OUTPUT_DIM_PREDICTOR = len(list(set(DEFENDER_SET) - set(ATTACKER_ACTION_SET)))

# ç”Ÿæˆå‚æ•°
NUM_TO_GENERATE = 40000
TRANSFER_ALPHA = 2.0
LSTM_FUSION_LAMBDA = 0.5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def main():
    set_seed(2025)
    print("=" * 60);
    print("ğŸš€ çº¯æ·±åº¦å­¦ä¹ ä¸‰çº§æ¡†æ¶ - STEP 3: ç”Ÿæˆæœ€ç»ˆä¼ªè£…æµé‡...");
    print("=" * 60)
    print(f"   (LSTMèåˆç³»æ•° Lambda: {LSTM_FUSION_LAMBDA})")

    print("\n[æ­¥éª¤1] åŠ è½½å…¨éƒ¨ä¸‰ä¸ªDLæ¨¡å‹åŠæ•°æ®...")
    scaler = joblib.load(SCALER_PATH)

    # âœ… 5. ä¿®æ”¹: åŠ è½½LSTMPredictoræ¨¡å‹ï¼Œè€Œä¸æ˜¯XGBoost
    predictor = LSTMPredictor(INPUT_DIM_PREDICTOR, OUTPUT_DIM_PREDICTOR).to(device)
    predictor.load_state_dict(torch.load(PREDICTOR_MODEL_PATH, map_location=device))
    predictor.eval()

    cae_model = ConditionalAutoencoder(FEATURE_DIM_CAE, LATENT_DIM_CAE, NUM_CLASSES_CAE).to(device)
    cae_model.load_state_dict(torch.load(CAE_MODEL_PATH, map_location=device))
    cae_model.eval()

    lstm_finetuner = LSTMFinetuner(INPUT_DIM_LSTM_FINETUNER, OUTPUT_DIM_LSTM_FINETUNER).to(device)
    lstm_finetuner.load_state_dict(torch.load(LSTM_FINETUNER_MODEL_PATH, map_location=device))
    lstm_finetuner.eval()

    df_benign_source = pd.read_csv(BENIGN_SOURCE_PATH).head(NUM_TO_GENERATE)
    df_train_full = pd.read_csv(TRAIN_SET_PATH)
    print("âœ… æ‰€æœ‰èµ„äº§åŠ è½½å®Œæ¯•ã€‚")

    # --- æ­¥éª¤2 å’Œ æ­¥éª¤3 çš„é€»è¾‘ä¸ä¹‹å‰å®Œå…¨ç›¸åŒï¼Œæ— éœ€ä¿®æ”¹ ---
    # ... (ä»æ‚¨çš„åŸä»£ç ä¸­ç›´æ¥å¤åˆ¶å³å¯)
    print("\n[æ­¥éª¤2] TIER 1 (æˆ˜ç•¥å±‚): ä½¿ç”¨CAEè¿›è¡Œé£æ ¼è¿ç§»...")
    with torch.no_grad():
        df_benign_train = df_train_full[df_train_full['label'] == 0];
        benign_scaled = scaler.transform(df_benign_train[DEFENDER_SET]);
        df_benign_scaled = pd.DataFrame(benign_scaled, columns=DEFENDER_SET)
        benign_knowledge = df_benign_scaled[ATTACKER_KNOWLEDGE_SET].values;
        c_benign = torch.tensor([1.0, 0.0]).expand(len(benign_knowledge), -1).to(device)
        z_benign_mean = torch.mean(
            cae_model.encode(torch.tensor(benign_knowledge, dtype=torch.float32).to(device), c_benign), dim=0)
        df_bot_train = df_train_full[df_train_full['label'] == 1];
        bot_scaled = scaler.transform(df_bot_train[DEFENDER_SET]);
        df_bot_scaled = pd.DataFrame(bot_scaled, columns=DEFENDER_SET)
        bot_knowledge = df_bot_scaled[ATTACKER_KNOWLEDGE_SET].values;
        c_bot = torch.tensor([0.0, 1.0]).expand(len(bot_knowledge), -1).to(device)
        z_bot_mean = torch.mean(cae_model.encode(torch.tensor(bot_knowledge, dtype=torch.float32).to(device), c_bot),
                                dim=0)
        transfer_vector = z_bot_mean - z_benign_mean
    source_scaled = scaler.transform(df_benign_source[DEFENDER_SET]);
    df_source_scaled = pd.DataFrame(source_scaled, columns=DEFENDER_SET)
    X_source_knowledge = df_source_scaled[ATTACKER_KNOWLEDGE_SET].values;
    X_source_tensor = torch.tensor(X_source_knowledge, dtype=torch.float32).to(device)
    c_benign_source = torch.tensor([1.0, 0.0]).expand(len(X_source_tensor), -1).to(device);
    c_bot_target = torch.tensor([0.0, 1.0]).expand(len(X_source_tensor), -1).to(device)
    with torch.no_grad():
        z_fake_bot = cae_model.encode(X_source_tensor, c_benign_source) + TRANSFER_ALPHA * transfer_vector
        generated_knowledge_features_scaled = cae_model.decode(z_fake_bot, c_bot_target)
    print("âœ… 13ç»´'ç²—åŠ å·¥'æ ¸å¿ƒç‰¹å¾ç”Ÿæˆå®Œæ¯•ã€‚")

    print("\n[æ­¥éª¤3] TIER 2 (æˆ˜æœ¯å±‚): ä½¿ç”¨LSTMè¿›è¡Œç‰¹å¾ç²¾è°ƒ...")
    with torch.no_grad():
        input_for_lstm = generated_knowledge_features_scaled.unsqueeze(1)
        refined_action_features_scaled = lstm_finetuner(input_for_lstm)
        df_knowledge_scaled = pd.DataFrame(generated_knowledge_features_scaled.cpu().numpy(),
                                           columns=ATTACKER_KNOWLEDGE_SET)
        original_action_features_scaled = torch.tensor(df_knowledge_scaled[ATTACKER_ACTION_SET].values,
                                                       dtype=torch.float32).to(device)
        fused_action_features_scaled = (
                                                   1 - LSTM_FUSION_LAMBDA) * original_action_features_scaled + LSTM_FUSION_LAMBDA * refined_action_features_scaled
        fused_action_features_scaled = fused_action_features_scaled.cpu().numpy()
        fused_action_features_scaled = np.clip(fused_action_features_scaled, 0, 1)
    print("âœ… 9ç»´'èåˆå'çš„è¡ŒåŠ¨ç‰¹å¾ç”Ÿæˆå®Œæ¯•ã€‚")

    # --- æ­¥éª¤4 å’Œ æ­¥éª¤5 çš„é€»è¾‘è¢«å½»åº•é‡æ„ ---

    print("\n[æ­¥éª¤4] TIER 3 (æ‰§è¡Œå±‚): ä½¿ç”¨LSTMé¢„æµ‹è¡ç”Ÿç‰¹å¾...")
    with torch.no_grad():
        # âœ… 6. ä¸ºLSTMé¢„æµ‹å™¨å‡†å¤‡è¾“å…¥: [N, 1, 9], æ•°æ®å¿…é¡»æ˜¯scaled
        input_for_predictor = torch.tensor(fused_action_features_scaled, dtype=torch.float32).unsqueeze(1).to(device)

        # LSTMè¾“å‡ºé¢„æµ‹çš„14ç»´è¡ç”Ÿç‰¹å¾ (scaled)
        predicted_derived_features_scaled = predictor(input_for_predictor).cpu().numpy()
    print("âœ… 14ç»´è¡ç”Ÿç‰¹å¾é¢„æµ‹å®Œæ¯•ã€‚")

    print("\n[æ­¥éª¤5] æ‹¼æ¥ã€é€†å‘ç¼©æ”¾å¹¶ä¿å­˜æœ€ç»ˆæµé‡...")
    # âœ… 7. æ‹¼æ¥å®Œæ•´çš„23ç»´scaledç‰¹å¾
    df_fused_action = pd.DataFrame(fused_action_features_scaled, columns=ATTACKER_ACTION_SET)
    df_predicted_derived = pd.DataFrame(predicted_derived_features_scaled,
                                        columns=sorted(list(set(DEFENDER_SET) - set(ATTACKER_ACTION_SET))))

    df_final_scaled = pd.concat([df_fused_action, df_predicted_derived], axis=1)
    # ç¡®ä¿åˆ—çš„é¡ºåºä¸DEFENDER_SETå®Œå…¨ä¸€è‡´ï¼Œè¿™æ­¥è‡³å…³é‡è¦ï¼
    df_final_scaled = df_final_scaled[DEFENDER_SET]

    # âœ… 8. å¯¹å®Œæ•´çš„23ç»´scaledç‰¹å¾è¿›è¡Œä¸€æ¬¡æ€§é€†å‘ç¼©æ”¾
    final_features_unscaled = scaler.inverse_transform(df_final_scaled.values)
    df_final_unscaled = pd.DataFrame(final_features_unscaled, columns=DEFENDER_SET)

    df_final_unscaled.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"\nâœ… {len(df_final_unscaled)} æ¡'çº¯æ·±åº¦å­¦ä¹ æ¡†æ¶'ä¼ªè£…Botæµé‡å·²ä¿å­˜åˆ°: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    main()