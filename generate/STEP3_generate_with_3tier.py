# generate/STEP3_generate_with_3tier.py
# (FINAL VERSION: ADAPTIVE TSR 100:1 + CLUSTERED FOCUS + HARD CONSTRAINTS + WATERMARK)

import pandas as pd
import numpy as np
import os
import sys
import joblib
import torch
from sklearn.cluster import KMeans

# ==========================================================
# --- è·¯å¾„ä¿®æ­£ä¸æ¨¡å—å¯¼å…¥ ---
# ==========================================================
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

# ğŸ”¥ å¯¼å…¥ config ä»¥è·å–å½“å‰æ•°æ®é›†ä¿¡æ¯
import config
from models.style_transfer_cae import ConditionalAutoencoder
from models.lstm_finetuner import LSTMFinetuner
from models.lstm_predictor import LSTMPredictor
from config import DEFENDER_SET, ATTACKER_KNOWLEDGE_SET, ATTACKER_ACTION_SET, COMPLEX_SET, set_seed

# ==========================================================
# --- é…ç½®åŒº (è·¯å¾„ä¸æ¨¡å‹) ---
# ==========================================================
# è®­ç»ƒé›†è·¯å¾„ (ç”¨äºæå–è‰¯æ€§è½½ä½“å’ŒBoté£æ ¼)
CLEAN_DATA_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
# ğŸ”¥ æµ‹è¯•é›†è·¯å¾„ (æ–°å¢ï¼šç”¨äºä¾¦å¯ŸçœŸå®Botæ•°é‡ï¼Œè®¡ç®—å‹åˆ¶æ¯”)
TEST_DATA_PATH = os.path.join(project_root, 'data', 'splits', 'holdout_test_set.csv')

SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
CAE_MODEL_PATH = os.path.join(project_root, 'models', 'style_transfer_cae.pt')
LSTM_FINETUNER_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_finetuner.pt')
PREDICTOR_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_reconciliation_predictor.pt')

# ğŸ”¥ åŠ¨æ€è¾“å‡ºè·¯å¾„ï¼šæ ¹æ®æ•°æ®é›†åç§°è‡ªåŠ¨å‘½åï¼Œé˜²æ­¢è¦†ç›–
output_filename = f'final_camouflage_{config.CURRENT_DATASET}_TSR100.csv'
OUTPUT_CSV_PATH = os.path.join(project_root, 'data', 'generated', output_filename)

FEATURE_DIM_CAE = len(ATTACKER_KNOWLEDGE_SET)
LATENT_DIM_CAE = 5
NUM_CLASSES_CAE = 2
INPUT_DIM_LSTM_FINETUNER = len(ATTACKER_KNOWLEDGE_SET)
OUTPUT_DIM_LSTM_FINETUNER = len(ATTACKER_ACTION_SET)
INPUT_DIM_PREDICTOR = len(ATTACKER_ACTION_SET)
OUTPUT_DIM_PREDICTOR = len(COMPLEX_SET)

# --- æˆ˜æœ¯å‚æ•° ---
# æ³¨æ„ï¼šNUM_TO_GENERATE ä¸å†ç¡¬ç¼–ç ï¼Œè€Œæ˜¯ç”± calculate_adaptive_quantity() è®¡ç®—
TACTICAL_SUPPRESSION_RATIO = 100  # æ ¸å¿ƒæˆ˜æœ¯æŒ‡æ ‡ 100:1
TACTICAL_WINDOW_CAP_2018 = 1000  # 2018æ•°æ®é›†çš„æˆ˜æœ¯çª—å£ä¸Šé™ (åªå‹åˆ¶å‰1000ä¸ªBot)

# æ¨¡ä»¿å¼ºåº¦ (0.98)
MIMIC_INTENSITY = 0.98

# Bot èšç±»ç°‡æ•°
NUM_BOT_CLUSTERS = 5

# --- æ°´å°å‚æ•° (æº¯æºæ ¸å¿ƒ) ---
WATERMARK_KEY = 97
WATERMARK_FEATURE = 'Flow Duration'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ==========================================================
# --- è‡ªé€‚åº”æ•°é‡è®¡ç®—å‡½æ•° (ä¿®å¤ç‰ˆ) ---
# ==========================================================
def calculate_adaptive_quantity():
    """
    æ ¹æ®å½“å‰æ•°æ®é›†å’Œæµ‹è¯•é›†ä¸­çš„çœŸå®Botæ•°é‡ï¼Œè®¡ç®—ç¬¦åˆ 100:1 å‹åˆ¶æ¯”çš„ç”Ÿæˆæ•°é‡ã€‚
    åŒ…å«æ ‡ç­¾ç±»å‹è‡ªåŠ¨è¯†åˆ«å’Œæ•°é‡å…œåº•é€»è¾‘ã€‚
    """
    print(f"\nğŸ” [æˆ˜æœ¯ä¾¦å¯Ÿ] æ­£åœ¨åˆ†ææµ‹è¯•é›†: {config.CURRENT_DATASET} ...")

    if not os.path.exists(TEST_DATA_PATH):
        print(f"   -> âŒ è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•é›†æ–‡ä»¶: {TEST_DATA_PATH}")
        print("   -> âš ï¸ å¯ç”¨é»˜è®¤å…œåº•æ•°é‡: 40000")
        return 40000

    # è¯»å–æµ‹è¯•é›† Label
    try:
        # åªè¯»å– Label åˆ—ä»¥åŠ é€Ÿ
        df_test = pd.read_csv(TEST_DATA_PATH)
        # å…¼å®¹æ€§å¤„ç†ï¼šæ£€æŸ¥åˆ—åæ˜¯ 'Label' è¿˜æ˜¯ 'label'
        label_col = 'Label' if 'Label' in df_test.columns else 'label'

        # æ‰“å°ä¸€ä¸‹å½“å‰çš„æ ‡ç­¾åˆ†å¸ƒï¼Œæ–¹ä¾¿è°ƒè¯•
        unique_labels = df_test[label_col].unique()
        print(f"   -> DEBUG: æµ‹è¯•é›†åŒ…å«çš„æ ‡ç­¾ç±»å‹: {unique_labels}")

        # --- æ ¸å¿ƒä¿®å¤: å¤šé‡åŒ¹é…é€»è¾‘ ---
        # 1. å°è¯•åŒ¹é…æ•°å­— 1
        real_bot_count = len(df_test[df_test[label_col] == 1])

        # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…å­—ç¬¦ä¸² 'Bot' (æˆ– config ä¸­å®šä¹‰çš„ malicious label)
        if real_bot_count == 0:
            target_str = getattr(config, 'MALICIOUS_LABEL', 'Bot')  # é»˜è®¤ä¸º 'Bot'
            real_bot_count = len(df_test[df_test[label_col] == target_str])

        print(f"   -> ä¾¦æµ‹åˆ°æµ‹è¯•é›†ä¸­çœŸå® Bot æ•°é‡: {real_bot_count}")

        # --- å…œåº•é€»è¾‘ ---
        if real_bot_count == 0:
            print("   -> âš ï¸ è­¦å‘Š: æœªèƒ½æ£€æµ‹åˆ°ä»»ä½• Bot æ ·æœ¬ (å¯èƒ½æ˜¯æ ‡ç­¾ä¸åŒ¹é…æˆ–æµ‹è¯•é›†å…¨ä¸ºè‰¯æ€§)ã€‚")
            print("   -> âš ï¸ å¯ç”¨å¼ºåˆ¶å…œåº•æ¨¡å¼: é»˜è®¤ç”Ÿæˆ 40,000 æ¡ï¼Œä»¥é˜²æ­¢ç¨‹åºå´©æºƒã€‚")
            return 40000

        # --- æ­£å¸¸è®¡ç®—é€»è¾‘ ---
        target_num = 0

        if config.CURRENT_DATASET == 'CIC-IDS2017':
            # 2017: å…¨é‡å‹åˆ¶
            target_num = real_bot_count * TACTICAL_SUPPRESSION_RATIO
            print(f"   -> æˆ˜æœ¯æ¨¡å¼: å…¨é‡é¥±å’Œæ‰“å‡» (Full Scale)")

        elif config.CURRENT_DATASET == 'CSE-CIC-IDS2018':
            # 2018: æˆ˜æœ¯çª—å£é‡‡æ ·
            tactical_targets = min(real_bot_count, TACTICAL_WINDOW_CAP_2018)
            target_num = tactical_targets * TACTICAL_SUPPRESSION_RATIO
            print(f"   -> æˆ˜æœ¯æ¨¡å¼: æˆ˜æœ¯çª—å£å‹åˆ¶ (Tactical Window Cap: {TACTICAL_WINDOW_CAP_2018} Targets)")

        else:
            # é»˜è®¤
            target_num = 40000
            print("   -> æˆ˜æœ¯æ¨¡å¼: é»˜è®¤è®¾ç½®")

        print(f"   -> âš ï¸ æœ€ç»ˆç¡®å®šç”Ÿæˆæ•°é‡ (NUM_TO_GENERATE): {target_num}")
        return int(target_num)

    except Exception as e:
        print(f"   -> âŒ ä¾¦å¯Ÿé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        print("   -> âš ï¸ å¯ç”¨å¼‚å¸¸å…œåº•æ¨¡å¼: é»˜è®¤ç”Ÿæˆ 40,000 æ¡")
        return 40000


# ==========================================================
# --- æ°´å°æ³¨å…¥å‡½æ•° (ä¿æŒåŸæ ·) ---
# ==========================================================
def inject_watermark(df, key, feature_name):
    """
    åœ¨æŒ‡å®šç‰¹å¾ä¸­æ³¨å…¥æ¨¡è¿ç®—æ°´å° (LSB Steganography)
    é€»è¾‘: ä¿®æ”¹æ•°å€¼ï¼Œä½¿å…¶ % key == 0
    """
    print(f"\nğŸŒŠ [æ­¥éª¤7] æ­£åœ¨æ³¨å…¥æº¯æºæ°´å° (Key={key}, Feature={feature_name})...")

    # å¤åˆ¶ä¸€ä»½ä»¥å…å½±å“åŸæ•°æ®æŒ‡é’ˆ
    df_w = df.copy()

    # è·å–åŸå§‹å€¼å¹¶è½¬ä¸ºæ•´æ•° (å¾®ç§’çº§æ—¶é—´æˆ³æœ¬èº«å°±æ˜¯æ•´æ•°)
    values = df_w[feature_name].values.astype(int)

    # è®¡ç®—ä½™æ•° (Residuals)
    residuals = values % key

    # ä¿®æ”¹å€¼: å‡å»ä½™æ•°ï¼Œä½¿å…¶èƒ½è¢« key æ•´é™¤
    new_values = values - residuals

    # ä¿®æ­£è¾¹ç•Œæƒ…å†µ: Duration ä¸èƒ½ä¸º 0 æˆ–è´Ÿæ•°
    # å¦‚æœå‡å»ä½™æ•°å <= 0ï¼Œåˆ™åŠ ä¸€ä¸ª Keyï¼Œä¿è¯å®ƒæ˜¯æ­£æ•°ä¸”ä¾ç„¶èƒ½è¢« Key æ•´é™¤
    mask_too_small = (new_values <= 0)
    new_values[mask_too_small] += key

    df_w[feature_name] = new_values

    # éªŒè¯æ³¨å…¥ç‡
    success_rate = np.mean(df_w[feature_name] % key == 0)
    print(f"   -> æ°´å°æ³¨å…¥å®Œæˆã€‚ç†è®ºéªŒè¯é€šè¿‡ç‡: {success_rate * 100:.2f}%")

    # âš ï¸ å…³é”®æ­¥éª¤: é‡æ–°è®¡ç®—é€Ÿç‡ç‰¹å¾ä»¥ä¿æŒç¡¬çº¦æŸè‡ªæ´½
    # å› ä¸º Flow Duration å˜äº†ï¼ŒBytes/s å’Œ Pkts/s å¿…é¡»åŒæ­¥å˜
    print("   -> æ­£åœ¨åŒæ­¥æ›´æ–°å…³è”ç‰¹å¾ (Bytes/s, Pkts/s) ä»¥ç»´æŒæ•°å­¦è‡ªæ´½...")

    duration_sec = df_w['Flow Duration'] / 1e6  # å¾®ç§’è½¬ç§’

    if 'Total Length of Fwd Packets' in df_w.columns:
        total_bytes = df_w['Total Length of Fwd Packets'] + df_w['Total Length of Bwd Packets']
        df_w['Flow Bytes/s'] = total_bytes / (duration_sec + 1e-9)

    if 'Total Fwd Packets' in df_w.columns:
        total_pkts = df_w['Total Fwd Packets'] + df_w['Total Backward Packets']
        df_w['Flow Packets/s'] = total_pkts / (duration_sec + 1e-9)

    return df_w


# ==========================================================
# --- ä¸»å‡½æ•° ---
# ==========================================================
def main():
    set_seed(2025)

    # ğŸ”¥ æ­¥éª¤0: è‡ªé€‚åº”è®¡ç®—ç”Ÿæˆæ•°é‡
    NUM_TO_GENERATE = calculate_adaptive_quantity()

    # å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢ç”Ÿæˆæ•°ä¸º0
    if NUM_TO_GENERATE <= 0:
        print("âŒ é”™è¯¯: ç”Ÿæˆæ•°é‡ä¸º 0ï¼Œå¼ºåˆ¶é€€å‡ºä»¥é¿å…æŠ¥é”™ã€‚")
        return

    print("=" * 60)
    print(f"ğŸš€ (Decoy + ClusterFocus + Traceability) STEP 3: ç”Ÿæˆ ({config.CURRENT_DATASET})...")
    print("=" * 60)
    print(f"   ç”Ÿæˆæ•°é‡: {NUM_TO_GENERATE} (Based on 100:1 TSR)")
    print(f"   æ¨¡ä»¿å¼ºåº¦: {MIMIC_INTENSITY}")
    print(f"   Botèšç±»æ•°: {NUM_BOT_CLUSTERS}")
    print(f"   æº¯æºå¯†é’¥: {WATERMARK_KEY}")

    # --- 1. åŠ è½½æ¨¡å‹åŠæ•°æ® ---
    print("\n[æ­¥éª¤1] åŠ è½½æ¨¡å‹åŠæ¸…æ´—åçš„æ•°æ®...")
    scaler = joblib.load(SCALER_PATH)

    predictor = LSTMPredictor(INPUT_DIM_PREDICTOR, OUTPUT_DIM_PREDICTOR).to(device)
    # å¢åŠ  weights_only=False ä»¥å…¼å®¹æ—§ç‰ˆ PyTorch ä¿å­˜çš„æ¨¡å‹ï¼Œé˜²æ­¢ FutureWarning åˆ·å±
    # å¦‚æœä½ çš„ PyTorch ç‰ˆæœ¬è¾ƒæ–°ä¸”æ¨¡å‹æ˜¯æ–°çš„ï¼Œå¯ä»¥å°è¯•å»æ‰ï¼Œä½†ä¸ºäº†ç¨³å¦¥è¿™é‡Œå…ˆä¸ç®¡è­¦å‘Š
    predictor.load_state_dict(torch.load(PREDICTOR_MODEL_PATH, map_location=device))
    predictor.eval()

    cae_model = ConditionalAutoencoder(FEATURE_DIM_CAE, LATENT_DIM_CAE, NUM_CLASSES_CAE).to(device)
    cae_model.load_state_dict(torch.load(CAE_MODEL_PATH, map_location=device))
    cae_model.eval()

    lstm_finetuner = LSTMFinetuner(INPUT_DIM_LSTM_FINETUNER, OUTPUT_DIM_LSTM_FINETUNER).to(device)
    lstm_finetuner.load_state_dict(torch.load(LSTM_FINETUNER_MODEL_PATH, map_location=device))
    lstm_finetuner.eval()

    df_clean_full = pd.read_csv(CLEAN_DATA_PATH)

    # 1.1 å‡†å¤‡ Benign æ¯ä½“
    # ä½¿ç”¨è®¡ç®—å‡ºçš„ NUM_TO_GENERATE
    # ğŸ”¥ å…³é”®ä¿®å¤: å¦‚æœ NUM_TO_GENERATE > 0 æ‰èƒ½é‡‡æ ·ï¼Œå¦åˆ™ä¼šæŠ¥é”™
    print(f"   -> æ­£åœ¨ä»èƒŒæ™¯æµé‡ä¸­é‡‡æ · {NUM_TO_GENERATE} æ¡ä½œä¸ºè½½ä½“...")
    df_benign_source = df_clean_full[df_clean_full['label'] == 0].sample(n=NUM_TO_GENERATE, replace=True,
                                                                         random_state=2025)

    # 1.2 å‡†å¤‡ Bot å…¨é‡æ•°æ® (ç”¨äºèšç±»)
    df_bot_all = df_clean_full[df_clean_full['label'] == 1]

    # é’ˆå¯¹ IDS2018 æ•°æ®é‡è¿‡å¤§çš„ä¼˜åŒ–ï¼šå¦‚æœBotå¤ªå¤šï¼Œèšç±»æ—¶é‡‡æ ·ä¸€ä¸‹ä»¥æé€Ÿ (ä¸å½±å“åç»­é€»è¾‘)
    if len(df_bot_all) > 20000:
        print(f"   -> (ä¼˜åŒ–) Botæ ·æœ¬è¿‡å¤š ({len(df_bot_all)})ï¼Œé‡‡æ · 20,000 ä¸ªç”¨äºæå–èšç±»é£æ ¼...")
        df_bot_all_for_cluster = df_bot_all.sample(n=20000, random_state=2025)
    else:
        df_bot_all_for_cluster = df_bot_all

    print(f"âœ… å‡†å¤‡å®Œæ¯•: {len(df_benign_source)} Benign æ¯ä½“, {len(df_bot_all_for_cluster)} çœŸå® Bot æ ·æœ¬(ç”¨äºèšç±»)ã€‚")

    # --- 1.5 Bot é£æ ¼èšç±» (å¯»æ‰¾æœ€å¼ºç‰¹å¾) ---
    print(f"\n[æ­¥éª¤1.5] å¯¹çœŸå® Bot è¿›è¡Œèšç±» (K={NUM_BOT_CLUSTERS}) ä»¥æå–çº¯ç²¹é£æ ¼...")

    # ç¼©æ”¾ Bot æ•°æ®
    bot_scaled_full = scaler.transform(df_bot_all_for_cluster[DEFENDER_SET])

    # æ‰§è¡Œ KMeans
    kmeans = KMeans(n_clusters=NUM_BOT_CLUSTERS, random_state=2025, n_init=10)
    kmeans.fit(bot_scaled_full)

    # è·å–èšç±»ä¸­å¿ƒ (ScaledçŠ¶æ€)
    centers_scaled = kmeans.cluster_centers_

    # å°†ä¸­å¿ƒé€†å‘ç¼©æ”¾å›åŸå§‹ç©ºé—´ï¼Œæ„å»º DataFrame
    centers_unscaled = scaler.inverse_transform(centers_scaled)
    df_bot_centers = pd.DataFrame(centers_unscaled, columns=DEFENDER_SET)

    print(f"   -> æˆåŠŸæå– {NUM_BOT_CLUSTERS} ä¸ª Bot é£æ ¼ä¸­å¿ƒã€‚")

    # éšæœºåˆ†é…å¯¼å¸ˆï¼šè®© NUM_TO_GENERATE ä¸ªæ¯ä½“éšæœºé€‰æ‹©è¿™ 5 ä¸ªä¸­å¿ƒä¹‹ä¸€è¿›è¡Œæ¨¡ä»¿
    tutor_indices = np.random.randint(0, NUM_BOT_CLUSTERS, size=NUM_TO_GENERATE)
    df_bot_tutors = df_bot_centers.iloc[tutor_indices].reset_index(drop=True)

    print(f"   -> å¯¼å¸ˆåˆ†é…å®Œæ¯•: æ‰€æœ‰ç”Ÿæˆæ ·æœ¬å°†å¼ºåˆ¶æ¨¡ä»¿è¿™ {NUM_BOT_CLUSTERS} ä¸ªä¸­å¿ƒã€‚")

    # --- 2. å¼ºåŠ›é£æ ¼æ¤å…¥ (TIER 1) ---
    print("\n[æ­¥éª¤2] TIER 1: æ‰§è¡Œç‚¹å¯¹ç‚¹é£æ ¼æ¤å…¥...")
    with torch.no_grad():
        # 2.1 Benign Z (Source)
        source_scaled = scaler.transform(df_benign_source[DEFENDER_SET])
        df_source_scaled = pd.DataFrame(source_scaled, columns=DEFENDER_SET)
        X_benign = torch.tensor(df_source_scaled[ATTACKER_KNOWLEDGE_SET].values, dtype=torch.float32).to(device)
        c_benign = torch.tensor([1.0, 0.0], dtype=torch.float32).expand(len(X_benign), -1).to(device)
        z_benign = cae_model.encode(X_benign, c_benign)

        # 2.2 Bot Z (Centers as Tutors)
        tutors_scaled = scaler.transform(df_bot_tutors[DEFENDER_SET])
        df_tutors_scaled = pd.DataFrame(tutors_scaled, columns=DEFENDER_SET)
        X_bot = torch.tensor(df_tutors_scaled[ATTACKER_KNOWLEDGE_SET].values, dtype=torch.float32).to(device)
        c_bot_input = torch.tensor([0.0, 1.0], dtype=torch.float32).expand(len(X_bot), -1).to(device)
        z_bot = cae_model.encode(X_bot, c_bot_input)

        # 2.3 æ··åˆ (MIMIC_INTENSITY = 0.98)
        # æåº¦åå‘ Botï¼ŒBenign åªæä¾›æå¾®å°çš„æ‰°åŠ¨
        z_hybrid = (1 - MIMIC_INTENSITY) * z_benign + MIMIC_INTENSITY * z_bot

        # 2.4 è§£ç 
        c_bot_target = torch.tensor([0.0, 1.0], dtype=torch.float32).expand(len(z_hybrid), -1).to(device)
        generated_knowledge_features_scaled = cae_model.decode(z_hybrid, c_bot_target)

    # --- 3. LSTM ç²¾è°ƒ (TIER 2) ---
    print("\n[æ­¥éª¤3] TIER 2: LSTM æˆ˜æœ¯å¾®è°ƒ...")
    with torch.no_grad():
        input_for_lstm = generated_knowledge_features_scaled.unsqueeze(1)
        refined_action = lstm_finetuner(input_for_lstm)

        df_knowledge_scaled = pd.DataFrame(generated_knowledge_features_scaled.cpu().numpy(),
                                           columns=ATTACKER_KNOWLEDGE_SET)
        original_action = torch.tensor(df_knowledge_scaled[ATTACKER_ACTION_SET].values, dtype=torch.float32).to(device)

        # èåˆ: LSTM çš„æƒé‡ä¿æŒ 0.7
        fused_action = 0.3 * original_action + 0.7 * refined_action
        fused_action = np.clip(fused_action.cpu().numpy(), 0, 1)

    # --- 4. è¡ç”Ÿç‰¹å¾é¢„æµ‹ (TIER 3) ---
    print("\n[æ­¥éª¤4] TIER 3: è¡ç”Ÿç‰¹å¾é¢„æµ‹...")
    with torch.no_grad():
        input_predictor = torch.tensor(fused_action, dtype=torch.float32).unsqueeze(1).to(device)
        predicted_complex = predictor(input_predictor).cpu().numpy()
        predicted_complex = np.clip(predicted_complex, 0, 1)

    # --- 5. é€†å‘ç¼©æ”¾ ---
    print("\n[æ­¥éª¤5] é€†å‘ç¼©æ”¾...")
    df_temp_action = pd.DataFrame(0, index=range(NUM_TO_GENERATE), columns=DEFENDER_SET)
    df_temp_action[ATTACKER_ACTION_SET] = fused_action
    action_unscaled = pd.DataFrame(scaler.inverse_transform(df_temp_action), columns=DEFENDER_SET)[ATTACKER_ACTION_SET]

    df_temp_complex = pd.DataFrame(0, index=range(NUM_TO_GENERATE), columns=DEFENDER_SET)
    df_temp_complex[COMPLEX_SET] = predicted_complex
    complex_unscaled = pd.DataFrame(scaler.inverse_transform(df_temp_complex), columns=DEFENDER_SET)[COMPLEX_SET]

    df_final = pd.concat([action_unscaled, complex_unscaled], axis=1)

    # --- 6. ç¡¬çº¦æŸæ ¡å‡† ---
    print("\n[æ­¥éª¤6] åº”ç”¨ç¡¬çº¦æŸ (åˆæ¬¡æ ¡å‡†)...")
    # åŸºç¡€è®¡ç®—
    df_final['Total Fwd Packets'] = df_final['Total Fwd Packets'].clip(lower=1)
    df_final['Total Backward Packets'] = df_final['Total Backward Packets'].clip(lower=0)
    df_final['Average Packet Size'] = df_final['Average Packet Size'].clip(lower=0)

    df_final['Total Length of Fwd Packets'] = df_final['Total Fwd Packets'] * df_final['Average Packet Size']
    df_final['Total Length of Bwd Packets'] = df_final['Total Backward Packets'] * df_final['Average Packet Size']

    total_pkts = df_final['Total Fwd Packets'] + df_final['Total Backward Packets']
    total_len = df_final['Total Length of Fwd Packets'] + df_final['Total Length of Bwd Packets']
    df_final['Packet Length Mean'] = total_len / (total_pkts + 1e-9)

    df_final['Flow Duration'] = df_final['Flow Duration'].clip(lower=1)
    duration_sec = df_final['Flow Duration'] / 1e6
    df_final['Flow Bytes/s'] = total_len / (duration_sec + 1e-9)
    df_final['Flow Packets/s'] = total_pkts / (duration_sec + 1e-9)
    df_final['Down/Up Ratio'] = df_final['Total Backward Packets'] / (df_final['Total Fwd Packets'] + 1e-9)

    # æå€¼ä¿®æ­£
    cols_root = ['Fwd Packet Length', 'Bwd Packet Length', 'Flow IAT', 'Fwd IAT', 'Bwd IAT']
    for root in cols_root:
        if f'{root} Min' in df_final.columns and f'{root} Max' in df_final.columns:
            df_final[f'{root} Min'] = df_final[f'{root} Min'].clip(lower=0)
            df_final[f'{root} Max'] = np.maximum(df_final[f'{root} Max'], df_final[f'{root} Min'])
            if f'{root} Mean' in df_final.columns:
                df_final[f'{root} Mean'] = np.clip(df_final[f'{root} Mean'], df_final[f'{root} Min'],
                                                   df_final[f'{root} Max'])

    # è¡¥å…¨åˆ—
    for col in DEFENDER_SET:
        if col not in df_final.columns:
            df_final[col] = 0
    df_final = df_final[DEFENDER_SET]

    # --- 7. æ³¨å…¥æº¯æºæ°´å° (å…³é”®æ­¥éª¤) ---
    df_final_watermarked = inject_watermark(df_final, WATERMARK_KEY, WATERMARK_FEATURE)

    # --- ä¿å­˜ ---
    df_final_watermarked.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"\nâœ… {len(df_final_watermarked)} æ¡'èšç±»èšç„¦+å¯æº¯æº'è¯±é¥µæµé‡å·²ä¿å­˜åˆ°: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    main()